\documentclass[a4paper,11pt]{article}
\pdfoutput=1 % if your are submitting a pdflatex (i.e. if you have
             % images in pdf, png or jpg format)

\usepackage{jheppub} % for details on the use of the package, please
                     % see the JHEP-author-manual

\usepackage[T1]{fontenc} % if needed



\title{\boldmath Time Series Analysis Basics}


%% %simple case: 2 authors, same institution
%% \author{A. Uthor}
%% \author{and A. Nother Author}
%% \affiliation{Institution,\\Address, Country}

% more complex case: 4 authors, 3 institutions, 2 footnotes
\author[a,b,1]{Elliot Golias,\note{Corresponding author.}}
%\author[c]{S. Econd,}
%\author[a,2]{T. Hird\note{Also at Some University.}}
%\author[a,2]{and Fourth}

% The "\note" macro will give a warning: "Ignoring empty anchor..."
% you can safely ignore it.

\affiliation[a]{Case Western Reserve University,\\some-street, Country}
%\affiliation[b]{Another University,\\different-address, Country}
%\affiliation[c]{A School for Advanced Studies,\\some-location, Country}

% e-mail addresses: one for each author, in the same order as the authors
\emailAdd{elliotgolias@case.edu}




\abstract{Abstract...}



\begin{document} 
\maketitle
\flushbottom

\section{Introduction to Time Series}
%%%%%%%%%%%%%%%%%%%%%%%%%
A time series is simply a collection of data where each element in the dataset has a corresponding time associated with it.

A \textit{statistically stationary} time series is a 

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Random Walks}
%%%%%%%%%%%%%%%%%%%%%%%
%
A \textit{random walk} is a state process $\{x_k\}$ satisfying
%
\begin{align}
	x_k = x_{k-1} + \omega_k,
\end{align}
%
where $\{\omega_k\}$ are iid Gaussian $\mathcal{N}(0, 1)$ and constitute a discrete white noise time series.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Autoregressive Models}
%%%%%%%%%%%%%%%%%%%%%%%%%
%
An \textit{autoregressive model} is a model consisting of a dependent variable that is dependent on one or more of previous values of itself. In particular, an autoregressive model of order $p$, denoted $AR(p)$, is defined by the equation
%
\begin{align}
X_t = c + \sum^p_{i=1} \phi_i X_{t-i} + \omega_t,
\end{align}
%
where $\phi_1, \dots, \phi_p$ are the parameters of the model, $c$ is a constant, and $\omega_t$ is white noise.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Moving Average Models}
%%%%%%%%%%%%%%%%%%%%%%%%
%
A \textit{moving-average model} is very similar to an autoregressive model, but, instead of the process being dependent on only previous values of itself, it is also dependent on a linear combination of past values of the white noise. In particular, we have
%
\begin{align}
	x_t = \omega_t + \beta_1 \omega_{t-1} + \dots \beta_p \omega_{t-p}.
\end{align}
%
%%%%%%%%%%%%%%%%%%%%%%%
\section{Autoregressive Moving Average Models}
%%%%%%%%%%%%%%%%%%%%%%
%
\begin{align}
	x_t &= \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + \dots + \omega_t + \beta_1 \omega_{t-1} + \beta_2 \omega_{t-2} \dots \beta_q \omega_{t-q} \nonumber \\
	&= \sum_{i=1}^p \alpha_i x_{t-i} + \omega_t + \sum_{i=1}^q \beta_i \omega_{t-i}
\end{align}
%
%\appendix

%\section{Some title}





\end{document}
